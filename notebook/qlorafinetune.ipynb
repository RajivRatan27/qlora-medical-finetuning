{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff7cd5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# STEP 1: Dependencies and Imports\n",
    "# ===============================\n",
    "# In a notebook, run this first:\n",
    "# !pip install -q torch transformers datasets peft accelerate bitsandbytes sentencepiece jsonlines rouge-score evaluate\n",
    "\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from rouge_score import rouge_scorer\n",
    "import pprint\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "\n",
    "# ===============================\n",
    "# STEP 2: Configuration\n",
    "# ===============================\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "OUT_DIR = \"outputs/llama3_medical_sft\"\n",
    "\n",
    "# --- Dataset Paths ---\n",
    "TRAIN_FILE = \"data/train.jsonl\"\n",
    "VAL_FILE = \"data/val.jsonl\"\n",
    "TEST_FILE = \"data/test.jsonl\"  # Must exist for the final evaluation step\n",
    "\n",
    "# --- Model & Training Params ---\n",
    "MAX_LENGTH = 2048\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EVAL_BATCH = 4\n",
    "GRAD_ACCUM = 8\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "TWO_STAGE_LR = False\n",
    "\n",
    "LOGGING_STEPS = 50\n",
    "SAVE_STEPS = 500\n",
    "\n",
    "SEED = 42\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ===============================\n",
    "# STEP 3: Data Collator for SFT\n",
    "# ===============================\n",
    "@dataclass\n",
    "class SFTCollator:\n",
    "    tokenizer\n",
    "    max_length: int = 2048\n",
    "\n",
    "    def __call__(self, features: List[Dict]):\n",
    "        texts = []\n",
    "        for f in features:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a clinical NLP assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{f.get('instruction', '')}\\n\\nINPUT:\\n{f.get('input', '')}\"},\n",
    "                {\"role\": \"assistant\", \"content\": str(f.get('output', ''))}\n",
    "            ]\n",
    "            txt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "            texts.append(txt)\n",
    "\n",
    "        batch = self.tokenizer(\n",
    "            texts, padding=True, truncation=True,\n",
    "            max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "# ===============================\n",
    "# STEP 4: Load Datasets\n",
    "# ===============================\n",
    "data_files = {\"train\": TRAIN_FILE, \"validation\": VAL_FILE}\n",
    "ds = load_dataset(\"json\", data_files=data_files)\n",
    "print(\"Loaded Training and Validation Datasets:\")\n",
    "print(ds)\n",
    "print(\"\\nSample:\", ds[\"train\"][0])\n",
    "\n",
    "# ===============================\n",
    "# STEP 5: Tokenizer & Quantization\n",
    "# ===============================\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"right\"\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# STEP 6: Load Base Model + LoRA\n",
    "# ===============================\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_cfg,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "base = prepare_model_for_kbit_training(base)\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=TARGET_MODULES, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# ===============================\n",
    "# STEP 7: Training\n",
    "# ===============================\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=SAVE_STEPS,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    optim=\"adamw_torch\",\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "collator = SFTCollator(tok, max_length=MAX_LENGTH)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator\n",
    ")\n",
    "\n",
    "if TWO_STAGE_LR:\n",
    "    trainer.create_optimizer_and_scheduler(trainer.get_num_train_steps())\n",
    "    def lr_lambda(step):\n",
    "        ratio = step / max(1, trainer.state.max_steps)\n",
    "        return (5e-6 if ratio < 0.5 else 1e-4) / 5e-6\n",
    "    trainer.lr_scheduler = LambdaLR(trainer.optimizer, lr_lambda)\n",
    "\n",
    "print(\"\\nStarting model training...\")\n",
    "train_result = trainer.train()\n",
    "trainer.save_state()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# ===============================\n",
    "# STEP 8: Save Final Model & Tokenizer\n",
    "# ===============================\n",
    "final_adapter_dir = os.path.join(OUT_DIR, \"final_adapter\")\n",
    "model.save_pretrained(final_adapter_dir)\n",
    "tok.save_pretrained(final_adapter_dir)\n",
    "print(f\"Saved final adapters and tokenizer to: {final_adapter_dir}\")\n",
    "\n",
    "# ===============================\n",
    "# STEP 9: Metrics Definitions\n",
    "# ===============================\n",
    "def _to_items(y):\n",
    "    try:\n",
    "        obj = y if isinstance(y, (list, dict)) else json.loads(y)\n",
    "    except Exception:\n",
    "        return []\n",
    "    items = []\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            if isinstance(v, list):\n",
    "                for vi in v: items.append((k, str(vi).lower()))\n",
    "            else:\n",
    "                items.append((k, str(v).lower()))\n",
    "    elif isinstance(obj, list):\n",
    "        for d in obj:\n",
    "            if isinstance(d, dict):\n",
    "                for k, v in d.items():\n",
    "                    if isinstance(v, list):\n",
    "                        for vi in v: items.append((k, str(vi).lower()))\n",
    "                    else:\n",
    "                        items.append((k, str(v).lower()))\n",
    "    return items\n",
    "\n",
    "def ner_prf1(preds, gts):\n",
    "    tp = fp = fn = 0\n",
    "    for p, g in zip(preds, gts):\n",
    "        pset, gset = Counter(_to_items(p)), Counter(_to_items(g))\n",
    "        for t in set(pset) | set(gset):\n",
    "            ctp = min(pset[t], gset[t])\n",
    "            tp += ctp\n",
    "            fp += max(pset[t] - ctp, 0)\n",
    "            fn += max(gset[t] - ctp, 0)\n",
    "    prec = tp / (tp + fp + 1e-12)\n",
    "    rec = tp / (tp + fn + 1e-12)\n",
    "    f1 = 2 * prec * rec / (prec + rec + 1e-12)\n",
    "    return {\"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "def rouge_scores(preds, refs):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    agg = {\"rouge1\": 0, \"rouge2\": 0, \"rougeL\": 0}\n",
    "    for p, r in zip(preds, refs):\n",
    "        s = scorer.score(r, p)\n",
    "        for k in agg: agg[k] += s[k].fmeasure\n",
    "    n = max(1, len(preds))\n",
    "    return {k: v / n for k, v in agg.items()}\n",
    "\n",
    "def _tok(s): return re.findall(r\"\\w+\", str(s).lower())\n",
    "\n",
    "def qa_accuracy_f1(preds, refs):\n",
    "    acc = sum(p.strip() == r.strip() for p, r in zip(preds, refs)) / max(1, len(preds))\n",
    "    tp = fp = fn = 0\n",
    "    for p, r in zip(preds, refs):\n",
    "        P, R = _tok(p), _tok(r)\n",
    "        Pset, Rset = Counter(P), Counter(R)\n",
    "        for w in set(Pset) | set(Rset):\n",
    "            ctp = min(Pset.get(w, 0), Rset.get(w, 0))\n",
    "            tp += ctp\n",
    "            fp += max(Pset.get(w, 0) - ctp, 0)\n",
    "            fn += max(Rset.get(w, 0) - ctp, 0)\n",
    "    prec = tp / (tp + fp + 1e-12)\n",
    "    rec = tp / (tp + fn + 1e-12)\n",
    "    f1 = 2 * prec * rec / (prec + rec + 1e-12)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# ===============================\n",
    "# STEP 10: Evaluate on Test Set\n",
    "# ===============================\n",
    "print(\"\\nStarting evaluation on the test set...\")\n",
    "model.eval()\n",
    "ds_test = load_dataset(\"json\", data_files={\"test\": TEST_FILE})[\"test\"]\n",
    "\n",
    "preds, refs, tasks = [], [], []\n",
    "for ex in ds_test:\n",
    "    msgs = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a clinical NLP assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{ex['instruction']}\\n\\nINPUT:\\n{ex.get('input', '')}\"}\n",
    "    ]\n",
    "    prompt = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        out_ids = model.generate(**inputs, max_new_tokens=256, temperature=0.0, do_sample=False)\n",
    "    out_text = tok.decode(out_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    pred = out_text.split(\"assistant\")[-1].strip()\n",
    "    preds.append(pred)\n",
    "    refs.append(str(ex[\"output\"]))\n",
    "    tasks.append(ex.get(\"task\", \"\"))\n",
    "\n",
    "# --- Compute metrics by task ---\n",
    "results = {}\n",
    "if any(t == \"ner\" for t in tasks):\n",
    "    idx = [i for i, t in enumerate(tasks) if t == \"ner\"]\n",
    "    results[\"NER\"] = ner_prf1([preds[i] for i in idx], [refs[i] for i in idx])\n",
    "if any(t == \"summarization\" for t in tasks):\n",
    "    idx = [i for i, t in enumerate(tasks) if t == \"summarization\"]\n",
    "    results[\"Summarization\"] = rouge_scores([preds[i] for i in idx], [refs[i] for i in idx])\n",
    "if any(t == \"med_qa\" for t in tasks):\n",
    "    idx = [i for i, t in enumerate(tasks) if t == \"med_qa\"]\n",
    "    results[\"MedicalQA\"] = qa_accuracy_f1([preds[i] for i in idx], [refs[i] for i in idx])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"           EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "pprint.pprint(results)\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
